{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from prophet import Prophet\n",
    "import pmdarima as pm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"cmdstanpy\").disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 8)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "colors = mpl.colormaps['coolwarm'].resampled(10)\n",
    "colors = colors(np.linspace(0, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Date</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>ZIP</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price_Each</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176558</td>\n",
       "      <td>1</td>\n",
       "      <td>4/19/19 8:46</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>75001</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>2</td>\n",
       "      <td>11.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176559</td>\n",
       "      <td>2</td>\n",
       "      <td>4/7/19 22:30</td>\n",
       "      <td>MA</td>\n",
       "      <td>Boston</td>\n",
       "      <td>2215</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176560</td>\n",
       "      <td>3</td>\n",
       "      <td>4/12/19 14:38</td>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>90001</td>\n",
       "      <td>Google Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>600.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176560</td>\n",
       "      <td>3</td>\n",
       "      <td>4/12/19 14:38</td>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>90001</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176561</td>\n",
       "      <td>4</td>\n",
       "      <td>4/30/19 9:27</td>\n",
       "      <td>CA</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>90001</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order_ID  CustomerID           Date State         City    ZIP  \\\n",
       "0    176558           1   4/19/19 8:46    TX       Dallas  75001   \n",
       "1    176559           2   4/7/19 22:30    MA       Boston   2215   \n",
       "2    176560           3  4/12/19 14:38    CA  Los Angeles  90001   \n",
       "3    176560           3  4/12/19 14:38    CA  Los Angeles  90001   \n",
       "4    176561           4   4/30/19 9:27    CA  Los Angeles  90001   \n",
       "\n",
       "                      Product  Quantity  Price_Each  \n",
       "0        USB-C Charging Cable         2       11.95  \n",
       "1  Bose SoundSport Headphones         1       99.99  \n",
       "2                Google Phone         1      600.00  \n",
       "3            Wired Headphones         1       11.99  \n",
       "4            Wired Headphones         1       11.99  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Anon_Cust_Order_Data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting\n",
    "\n",
    "The stats library in Python has tools for building several other time-series prediction models - ARMA, ARIMA, and SARIMA - with just a few lines of code. \n",
    "\n",
    "Since all of these models are available in a single library, we can easily run many Python forecasting experiments using different models in the same script or notebook when conducting time series forecasting in Python. \n",
    "\n",
    "https://builtin.com/data-science/time-series-forecasting-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with different levels of prediction:\n",
    "There is a tradeoff between granular prediction and speed of moel output. The goal of this algorithm is to find the best tradeoff between model prediction accuracy and cost of model (run time, resources). By presenting a range of outcomes, our client will have the ability to choose based on their business goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level 1: Separate Regionally:\n",
    "\n",
    "- Separate dataset by product and city.\n",
    "- Train predictive machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USB-C Charging Cable' 'Bose SoundSport Headphones' 'Google Phone'\n",
      " 'Wired Headphones' 'Macbook Pro Laptop' 'Lightning Charging Cable'\n",
      " '27in 4K Gaming Monitor' 'AA Batteries (4-pack)'\n",
      " 'Apple Airpods Headphones' 'AAA Batteries (4-pack)' 'iPhone'\n",
      " 'Flatscreen TV' '27in FHD Monitor' '20in Monitor' 'LG Dryer'\n",
      " 'ThinkPad Laptop' 'Vareebadd Phone' 'LG Washing Machine'\n",
      " '34in Ultrawide Monitor'] ['Dallas' 'Boston' 'Los Angeles' 'San Francisco' 'Seattle' 'Atlanta'\n",
      " 'New York City' 'Portland' 'Austin'] 171\n"
     ]
    }
   ],
   "source": [
    "prods = df['Product'].unique()\n",
    "cities = df['City'].unique()\n",
    "total = len(prods) * len(cities)\n",
    "print(prods, cities, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write programs for running several time-series predictive ML algorithms so we can compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARIMA_model (train_df, test_df, order0, order1, order2):\n",
    "\n",
    "    ARIMAmodel = ARIMA(train_df, order = (order0, order1, order2))\n",
    "    ARIMAmodel = ARIMAmodel.fit()\n",
    "\n",
    "    y_pred = ARIMAmodel.get_forecast(len(test_df.index))\n",
    "    y_pred_df = y_pred.conf_int(alpha = 0.05) \n",
    "    y_pred_df[\"Predictions\"] = ARIMAmodel.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])\n",
    "    y_pred_df.index = test_df.index\n",
    "    y_pred_out = y_pred_df[\"Predictions\"] \n",
    "    \n",
    "    #plt.plot(y_pred_out, color='Red', label = 'ARIMA Predictions')\n",
    "    # plt.show()\n",
    "\n",
    "    arima_rmse = np.sqrt(mean_squared_error(test_df[\"Quantity\"].values, y_pred_df[\"Predictions\"]))\n",
    "\n",
    "    return arima_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARIMAX_model (train_df, test_df, order0, order1, order2, order3):\n",
    "    \n",
    "    SARIMAXmodel = SARIMAX(train_df, seasonal_order=(order0, order1, order2, order3))\n",
    "    SARIMAXmodel = SARIMAXmodel.fit(disp=False)\n",
    "\n",
    "    y_pred = SARIMAXmodel.get_forecast(len(test_df.index))\n",
    "    y_pred_df = y_pred.conf_int(alpha = 0.05) \n",
    "    y_pred_df[\"Predictions\"] = SARIMAXmodel.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])\n",
    "    y_pred_df.index = test_df.index\n",
    "    y_pred_out = y_pred_df[\"Predictions\"] \n",
    "   \n",
    "    #plt.plot(y_pred_out, color='Blue', label = 'SARIMA Predictions')\n",
    "    #plt.show()\n",
    "    \n",
    "    sarimax_rmse = np.sqrt(mean_squared_error(test_df[\"Quantity\"].values, y_pred_df[\"Predictions\"]))\n",
    "\n",
    "    return sarimax_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prophet(p_df):\n",
    "\n",
    "    p_df[\"Date\"] = p_df[\"Date\"].dt.to_period(\"W\").dt.start_time\n",
    "    \n",
    "    p_df = p_df.groupby('Date', as_index=False).agg('sum')\n",
    "    \n",
    "    p_df.columns = ['ds', 'y']\n",
    "    \n",
    "    p_train = p_df[p_df.index <= 40]\n",
    "    p_test = p_df[p_df.index > 40]\n",
    "    \n",
    "    p_model = Prophet()\n",
    "    \n",
    "    if p_train.shape[0] > 0 and p_test.shape[0] > 0:\n",
    "        p_model.fit(p_train)\n",
    "        p_forecast = p_model.predict(p_test)\n",
    "        \n",
    "        ## Calculate RMSE:\n",
    "        y_actual = p_test['y']\n",
    "        y_predicted = p_forecast['yhat']\n",
    "    \n",
    "        p_rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    \n",
    "    else:\n",
    "         p_rmse = 1000 \n",
    "    return p_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2matrix(dnn_arr, look_back):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(len(dnn_arr)-look_back):\n",
    "        d = i+look_back\n",
    "        X.append(dnn_arr[i:d, 0])\n",
    "        Y.append(dnn_arr[d, 0])\n",
    "    \n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_DNN (dnn_df):\n",
    "\n",
    "    train_size = 40\n",
    "\n",
    "    d_train, d_test = dnn_df.values[0:train_size,:], dnn_df.values[train_size:len(dnn_df.values),:]\n",
    "    \n",
    "    look_back = 4\n",
    "\n",
    "    trainX, trainY = convert2matrix(d_train, look_back)\n",
    "    testX, testY = convert2matrix(d_test, look_back)\n",
    "\n",
    "\n",
    "    d_model= Sequential()\n",
    "    d_model.add(Dense(units=32, input_dim=look_back, activation='relu'))\n",
    "    d_model.add(Dense(8, activation='relu'))\n",
    "    d_model.add(Dense(1))\n",
    "    d_model.compile(loss='mean_squared_error',  optimizer='adam',metrics = ['mse', 'mae'])\n",
    "    \n",
    "    history = d_model.fit(trainX,trainY, epochs=10, batch_size=None, verbose=0, validation_data=(testX,testY),shuffle=False)\n",
    "\n",
    "    #d_forecast = d_model.predict(testX)\n",
    "    test_score = d_model.evaluate(testX, testY, verbose=0)\n",
    "    \n",
    "\n",
    "    ## Calculate RMSE:\n",
    "    if len(test_score) >0:\n",
    "        d_rmse = np.sqrt(test_score[1])\n",
    "    else:\n",
    "        d_rmse = 1000\n",
    "\n",
    "    return d_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestARModel():\n",
    "    \"\"\"\n",
    "    Autoregressive forecasting with Random Forests\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_lags=1, max_depth = 3, n_estimators=1000, random_state = 123,\n",
    "                 log_transform = False, first_differences = False, seasonal_differences = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_lags: Number of lagged features to consider in autoregressive model\n",
    "            max_depth: Max depth for the forest's regression trees\n",
    "            random_state: Random state to pass to random forest\n",
    "            \n",
    "            log_transform: Whether the input should be log-transformed\n",
    "            first_differences: Whether the input should be singly differenced\n",
    "            seasonal_differences: Seasonality to consider, if 'None' then no seasonality is presumed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_lags = n_lags\n",
    "        self.model = RandomForestRegressor(max_depth = max_depth, n_estimators = n_estimators, random_state = random_state)\n",
    "        \n",
    "        self.log_transform = log_transform\n",
    "        self.first_differences = first_differences\n",
    "        self.seasonal_differences = seasonal_differences\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y: training data (numpy array or pandas series/dataframe)\n",
    "        \"\"\"\n",
    "        #enable pandas functions via dataframes\n",
    "        y_df = pd.DataFrame(y)\n",
    "        self.y_df = deepcopy(y_df)\n",
    "        \n",
    "        #apply transformations and store results for retransformations\n",
    "        if self.log_transform:\n",
    "            y_df = np.log(y_df)\n",
    "            self.y_logged = deepcopy(y_df)\n",
    "        \n",
    "        if self.first_differences:\n",
    "            y_df = y_df.diff().dropna()\n",
    "            self.y_diffed = deepcopy(y_df)\n",
    "        \n",
    "        if self.seasonal_differences is not None:\n",
    "            y_df = y_df.diff(self.seasonal_differences).dropna()\n",
    "            self.y_diffed_seasonal = deepcopy(y_df)\n",
    "        \n",
    "        \n",
    "        #get lagged features\n",
    "        Xtrain = pd.concat([y_df.shift(t) for t in range(1,self.n_lags+1)],axis=1).dropna()\n",
    "        self.Xtrain = Xtrain\n",
    "        \n",
    "        ytrain = y_df.loc[Xtrain.index,:]\n",
    "        self.ytrain = ytrain\n",
    "\n",
    "        self.model.fit(Xtrain.values,ytrain.values.reshape(-1))\n",
    "\n",
    "    \n",
    "    \n",
    "    def sample_forecast(self, n_periods = 1, n_samples = 10000, random_seed =123):\n",
    "        \"\"\"\n",
    "        Draw forecasting samples by randomly drawing from all trees in the forest per forecast period\n",
    "        Args:\n",
    "            n_periods: Ammount of periods to forecast\n",
    "            n_samples: Number of samples to draw\n",
    "            random_seed: Random seed for numpy\n",
    "        \"\"\"\n",
    "        samples = self._perform_forecast(n_periods, n_samples, random_seed)\n",
    "        output = self._retransform_forecast(samples, n_periods)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _perform_forecast(self, n_periods, n_samples, random_seed):\n",
    "        \"\"\"\n",
    "        Forecast transformed observations\n",
    "        Args:\n",
    "            n_periods: Ammount of periods to forecast\n",
    "            n_samples: Number of samples to draw\n",
    "            random_seed: Random seed for numpy\n",
    "        \"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        np.random.seed(random_seed)\n",
    "        for i in range(n_samples):\n",
    "            #store lagged features for each period\n",
    "            Xf = np.concatenate([self.Xtrain.iloc[-1,1:].values.reshape(1,-1),\n",
    "                                 self.ytrain.iloc[-1].values.reshape(1,1)],1)\n",
    "\n",
    "            forecasts = []\n",
    "\n",
    "            for t in range(n_periods):\n",
    "                tree = self.model.estimators_[np.random.randint(len(self.model.estimators_))]\n",
    "                pred = tree.predict(Xf)[0]\n",
    "                forecasts.append(pred)\n",
    "                \n",
    "                #update lagged features for next period\n",
    "                Xf = np.concatenate([Xf[:,1:],np.array([[pred]])],1)\n",
    "            \n",
    "            samples.append(forecasts)\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _retransform_forecast(self, samples, n_periods):\n",
    "        \"\"\"\n",
    "        Retransform forecast (re-difference and exponentiate)\n",
    "        Args:\n",
    "            samples: Forecast samples for retransformation\n",
    "            n_periods: Ammount of periods to forecast\n",
    "        \"\"\"\n",
    "        \n",
    "        full_sample_tree = []\n",
    "\n",
    "        for samp in samples:\n",
    "            draw = np.array(samp)\n",
    "            \n",
    "            #retransform seasonal differencing\n",
    "            if self.seasonal_differences is not None:\n",
    "                result = list(self.y_diffed.iloc[-self.seasonal_differences:].values)\n",
    "                for t in range(n_periods):\n",
    "                    result.append(result[t]+draw[t])\n",
    "                result = result[self.seasonal_differences:]\n",
    "            else:\n",
    "                result = []\n",
    "                for t in range(n_periods):\n",
    "                    result.append(draw[t])\n",
    "            \n",
    "            #retransform first differences\n",
    "            y_for_add = self.y_logged.values[-1] if self.log_transform else self.y_df.values[-1]\n",
    "            \n",
    "            if self.first_differences:\n",
    "                result = y_for_add + np.cumsum(result)\n",
    "            \n",
    "            #retransform log transformation\n",
    "            if self.log_transform:\n",
    "                result = np.exp(result)\n",
    "            \n",
    "            full_sample_tree.append(result.reshape(-1,1))\n",
    "\n",
    "        return np.concatenate(full_sample_tree,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile models into a baseline program that will run all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_baseline(df):\n",
    "    results = []\n",
    "    a_results = []\n",
    "    s_results = []\n",
    "    p_results = []\n",
    "    nnet_results = []\n",
    "    rf_results = []\n",
    "    \n",
    "    for i in range(len(prods)):\n",
    "        prod = prods[i]\n",
    "        \n",
    "        model_df = df[df['Product'] == prod]\n",
    "    \n",
    "        model_df['Date'] = pd.to_datetime(df['Date'])\n",
    "        prophet_df = model_df[['Date','Quantity']]\n",
    "\n",
    "        ## Group by Week:\n",
    "        #model_df['week'] = model_df['Date'].dt.week  #.dt.week was depreciated\n",
    "        model_df['week'] = model_df['Date'].dt.isocalendar().week\n",
    "            \n",
    "        grouped_df = model_df[['week','Quantity']]\n",
    "        nnet_df = grouped_df.groupby(['week'], as_index = False).agg('sum')   ## Add in df formatted for NNET\n",
    "        grouped_df = grouped_df.groupby(['week']).agg('sum')\n",
    "\n",
    "            \n",
    "        # Split into train / test set\n",
    "            \n",
    "        train = grouped_df[grouped_df.index <= 40]\n",
    "        test = grouped_df[grouped_df.index > 40]\n",
    "        y = train['Quantity']\n",
    "        print(\"Y equals \", y.head() )\n",
    "            \n",
    "        model = pm.auto_arima(y, \n",
    "                  seasonal=False,\n",
    "                  #trace=True,\n",
    "                  trace=False,\n",
    "                  error_action='ignore',  \n",
    "                  suppress_warnings=True, \n",
    "                  stepwise=True)\n",
    "\n",
    "        order0 = model.order[0]\n",
    "        order1 = model.order[1]\n",
    "        order2 = model.order[2]\n",
    "\n",
    "        rmse = ARIMA_model(train, test, order0, order1, order2)\n",
    "        a_results.append((prod, 'ARIMA('+','.join(map(str, model.order))+')', rmse))\n",
    "\n",
    "        s_model = pm.auto_arima(y, \n",
    "                  seasonal=True,\n",
    "                  m=12,\n",
    "                  #trace=True,\n",
    "                  trace=False,\n",
    "                  error_action='ignore',  \n",
    "                  suppress_warnings=True, \n",
    "                  stepwise=True)\n",
    "\n",
    "        s_order0 = s_model.order[0]\n",
    "        s_order1 = s_model.order[1]\n",
    "        s_order2 = s_model.order[2]\n",
    "        s_order3 = s_model.seasonal_order[3]\n",
    "        \n",
    "        # Run the model:\n",
    "        s_rmse = SARIMAX_model(train, test, s_order0, s_order1, s_order2, s_order3)\n",
    "        s_results.append((prod, 'SARIMAX('+','.join(map(str, s_model.seasonal_order))+')', s_rmse))\n",
    "\n",
    "\n",
    "        ## PROPHET:\n",
    "        p_rmse = run_prophet(prophet_df)\n",
    "        p_results.append((prod, 'PROPHET', p_rmse))\n",
    "\n",
    "        ## Deep Neural Net:\n",
    "        d_rmse = run_DNN(nnet_df)\n",
    "        nnet_results.append((prod, 'NNET', d_rmse))\n",
    "\n",
    "        ## Random Forest:\n",
    "        RandomForestmodel = RandomForestARModel(n_lags = 2, log_transform = True, first_differences = True, seasonal_differences = 12)\n",
    "        RandomForestmodel.fit(y)\n",
    "\n",
    "        y_pred = RandomForestmodel.sample_forecast(n_periods=len(test), n_samples=10000)\n",
    "        means_forest = np.mean(y_pred,1)\n",
    "\n",
    "        RandomForest_rmse = np.sqrt(mean_squared_error(test.values[:,0], means_forest))\n",
    "        rf_results.append((prod, 'RandomForestARModel', RandomForest_rmse))\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(len(a_results)):\n",
    "        if s_results[j][2] < a_results[j][2] and s_results[j][2] < p_results[j][2] and s_results[j][2] < nnet_results[j][2]:\n",
    "            results.append(s_results[j])\n",
    "        elif p_results[j][2] < a_results[j][2] and p_results[j][2] < s_results[j][2] and p_results[j][2] < nnet_results[j][2]:\n",
    "            results.append(p_results[j])\n",
    "        elif nnet_results[j][2] < a_results[j][2] and nnet_results[j][2] < s_results[j][2] and nnet_results[j][2] < p_results[j][2]:\n",
    "            results.append(nnet_results[j])\n",
    "        elif rf_results[j][2] < a_results[j][2] and rf_results[j][2] < s_results[j][2] and rf_results[j][2] < p_results[j][2]:\n",
    "            results.append(rf_results[j])\n",
    "        else:\n",
    "            results.append(a_results[j])\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING!\n",
    "\n",
    "def TEST_model_selection_baseline(df):\n",
    "    results = []\n",
    "    a_results = []\n",
    "    s_results = []\n",
    "    p_results = []\n",
    "    nnet_results = []\n",
    "    rf_results = []\n",
    "    \n",
    "    for i in range(len(prods)):\n",
    "        prod = prods[i]\n",
    "        \n",
    "        model_df = df[df['Product'] == prod]\n",
    "    \n",
    "        model_df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        \n",
    "        print(\"PRINTING HERE\")\n",
    "        display(model_df.head())\n",
    "        print(model_df.info())\n",
    "        \n",
    "        \n",
    "        prophet_df = model_df[['Date','Quantity']]\n",
    "\n",
    "        ## Group by Week:\n",
    "        \n",
    "        model_df['week'] = model_df['Date'].dt.isocalendar().week\n",
    "        \n",
    "        #model_df['week'] = model_df['Date'].dt.week\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        grouped_df = model_df[['week','Quantity']]\n",
    "        nnet_df = grouped_df.groupby(['week'], as_index = False).agg('sum')   ## Add in df formatted for NNET\n",
    "        grouped_df = grouped_df.groupby(['week']).agg('sum')\n",
    "\n",
    "        '''\n",
    "        # Split into train / test set\n",
    "            \n",
    "        train = grouped_df[grouped_df.index <= 40]\n",
    "        test = grouped_df[grouped_df.index > 40]\n",
    "        y = train['Quantity']\n",
    "        print(\"Y equals \", y.head() )\n",
    "            \n",
    "        model = pm.auto_arima(y, \n",
    "                  seasonal=False,\n",
    "                  #trace=True,\n",
    "                  trace=False,\n",
    "                  error_action='ignore',  \n",
    "                  suppress_warnings=True, \n",
    "                  stepwise=True)\n",
    "\n",
    "        order0 = model.order[0]\n",
    "        order1 = model.order[1]\n",
    "        order2 = model.order[2]\n",
    "\n",
    "        rmse = ARIMA_model(train, test, order0, order1, order2)\n",
    "        a_results.append((prod, 'ARIMA('+','.join(map(str, model.order))+')', rmse))\n",
    "\n",
    "        s_model = pm.auto_arima(y, \n",
    "                  seasonal=True,\n",
    "                  m=12,\n",
    "                  #trace=True,\n",
    "                  trace=False,\n",
    "                  error_action='ignore',  \n",
    "                  suppress_warnings=True, \n",
    "                  stepwise=True)\n",
    "\n",
    "        s_order0 = s_model.order[0]\n",
    "        s_order1 = s_model.order[1]\n",
    "        s_order2 = s_model.order[2]\n",
    "        s_order3 = s_model.seasonal_order[3]\n",
    "        \n",
    "        # Run the model:\n",
    "        s_rmse = SARIMAX_model(train, test, s_order0, s_order1, s_order2, s_order3)\n",
    "        s_results.append((prod, 'SARIMAX('+','.join(map(str, s_model.seasonal_order))+')', s_rmse))\n",
    "\n",
    "\n",
    "        ## PROPHET:\n",
    "        p_rmse = run_prophet(prophet_df)\n",
    "        p_results.append((prod, 'PROPHET', p_rmse))\n",
    "\n",
    "        ## Deep Neural Net:\n",
    "        d_rmse = run_DNN(nnet_df)\n",
    "        nnet_results.append((prod, 'NNET', d_rmse))\n",
    "\n",
    "        ## Random Forest:\n",
    "        RandomForestmodel = RandomForestARModel(n_lags = 2, log_transform = True, first_differences = True, seasonal_differences = 12)\n",
    "        RandomForestmodel.fit(y)\n",
    "\n",
    "        y_pred = RandomForestmodel.sample_forecast(n_periods=len(test), n_samples=10000)\n",
    "        means_forest = np.mean(y_pred,1)\n",
    "\n",
    "        RandomForest_rmse = np.sqrt(mean_squared_error(test.values[:,0], means_forest))\n",
    "        rf_results.append((prod, 'RandomForestARModel', RandomForest_rmse))\n",
    "\n",
    "\n",
    "\n",
    "    for j in range(len(a_results)):\n",
    "        if s_results[j][2] < a_results[j][2] and s_results[j][2] < p_results[j][2] and s_results[j][2] < nnet_results[j][2]:\n",
    "            results.append(s_results[j])\n",
    "        elif p_results[j][2] < a_results[j][2] and p_results[j][2] < s_results[j][2] and p_results[j][2] < nnet_results[j][2]:\n",
    "            results.append(p_results[j])\n",
    "        elif nnet_results[j][2] < a_results[j][2] and nnet_results[j][2] < s_results[j][2] and nnet_results[j][2] < p_results[j][2]:\n",
    "            results.append(nnet_results[j])\n",
    "        elif rf_results[j][2] < a_results[j][2] and rf_results[j][2] < s_results[j][2] and rf_results[j][2] < p_results[j][2]:\n",
    "            results.append(rf_results[j])\n",
    "        else:\n",
    "            results.append(a_results[j])\n",
    "        '''            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show RMSE tables by product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_by_prod (product_list, result_list):\n",
    "    prod_dict = {\"Product\":[], \"Model\": [],\"RMSE\": []}\n",
    "\n",
    "    for i in range(len(product_list)):\n",
    "        prod_name = product_list[i]\n",
    "\n",
    "        for j in range(len(result_list)):  \n",
    "            \n",
    "            if result_list[j][0] == prod_name:\n",
    "                prod_dict['Product'].append(result_list[j][0])\n",
    "                prod_dict[\"Model\"].append(result_list[j][1])\n",
    "                prod_dict[\"RMSE\"].append(result_list[j][2])\n",
    "    \n",
    "    return prod_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST_model_selection_baseline(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y equals  week\n",
      "1    467\n",
      "2    274\n",
      "3    305\n",
      "4    294\n",
      "5    338\n",
      "Name: Quantity, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 11:02:27.314813: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p5/k8h2_yjj0nncvy0f82hy1xvc0000gn/T/ipykernel_18426/2342949800.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaselines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/p5/k8h2_yjj0nncvy0f82hy1xvc0000gn/T/ipykernel_18426/1917291831.py\u001b[0m in \u001b[0;36mmodel_selection_baseline\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m## Deep Neural Net:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0md_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_DNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnet_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mnnet_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NNET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/p5/k8h2_yjj0nncvy0f82hy1xvc0000gn/T/ipykernel_18426/407470160.py\u001b[0m in \u001b[0;36mrun_DNN\u001b[0;34m(dnn_df)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0md_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#d_forecast = d_model.predict(testX)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "baselines = model_selection_baseline(df)\n",
    "baselines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
